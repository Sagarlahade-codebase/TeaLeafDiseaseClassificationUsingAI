# -*- coding: utf-8 -*-
"""FinalMLTeaLeafSagarLogin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17Mi3GABL0evmVzIHjJBcPKl0WJZKRJkE
"""

import numpy as np
from keras.models import load_model
from google.colab import drive
drive.mount('/content/drive')

from keras import backend as K
import tensorflow as tf
from sklearn.decomposition import PCA
import numpy as np


def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives +
    K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)

    return 2*((precision*recall)/(precision+recall+K.epsilon()))

from keras.utils import get_custom_objects
from keras.preprocessing.image import ImageDataGenerator

train_path = '/content/drive/MyDrive/TeaLeafDatasetForML22Mar/'
valid_path = '/content/drive/MyDrive/TeaLeafDatasetForML22Mar/'
test_path = '/content/drive/MyDrive/TeaLeafDatasetForML22Mar/'


get_custom_objects().update({'f1_m': f1_m,'precision_m':precision_m, 'recall_m':recall_m})

train_batches_vgg16 = ImageDataGenerator( rescale=1./255,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    vertical_flip=False,
    fill_mode='nearest',preprocessing_function=tf.keras.applications.vgg16.preprocess_input
).flow_from_directory(train_path, target_size=(224,224), batch_size=10,shuffle=False)
#valid_batches_vgg16 = ImageDataGenerator(rescale=1./255,preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(valid_path, target_size=(224,224), batch_size=10)
#test_batches_vgg16 = ImageDataGenerator(rescale=1./255,preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(test_path, target_size=(224,224), batch_size=10,shuffle=False)

train_batches_googlenet = ImageDataGenerator( rescale=1./255,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    vertical_flip=False,
    fill_mode='nearest',preprocessing_function=tf.keras.applications.vgg16.preprocess_input
).flow_from_directory(train_path, target_size=(224,224), batch_size=10,shuffle=False)
#valid_batches_googlenet = ImageDataGenerator(rescale=1./255,preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(valid_path, target_size=(224,224), batch_size=10)
#test_batches_googlenet = ImageDataGenerator(rescale=1./255,preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(test_path, target_size=(224,224), batch_size=10,shuffle=False)


train_batches_resnet = ImageDataGenerator(rescale=1./255,preprocessing_function=tf.keras.applications.resnet50.preprocess_input).flow_from_directory(train_path, target_size=(224,224), batch_size=10,shuffle=False)
#valid_batches_resnet = ImageDataGenerator(preprocessing_function=tf.keras.applications.resnet50.preprocess_input).flow_from_directory(valid_path, target_size=(224,224), batch_size=10)
#test_batches_resnet = ImageDataGenerator(rescale=1./255,preprocessing_function=tf.keras.applications.resnet50.preprocess_input).flow_from_directory(test_path, target_size=(224,224), batch_size=10,shuffle=False)

# Commented out IPython magic to ensure Python compatibility.



import cv2
import numpy as np
import os
from keras.preprocessing.image import ImageDataGenerator
from keras import backend as K
import keras
from keras.callbacks import LearningRateScheduler
from keras.metrics import categorical_crossentropy
from keras.layers import Activation
from keras.utils import get_custom_objects

import matplotlib.pyplot as plt
import os
import shutil
import random
import glob
import math
import tensorflow as tf
# %matplotlib inline

from keras.models import load_model
from keras.preprocessing.image import ImageDataGenerator
import sklearn.metrics as metrics
from keras.models import load_model
from sklearn.metrics import classification_report, confusion_matrix
from keras.layers import Flatten, Reshape
from keras.utils import Sequence
from keras.activations import softmax
from sklearn.preprocessing import MinMaxScaler
from keras.layers import concatenate, Dense, Input, GlobalAveragePooling2D
from keras.layers import Concatenate

import keras
from keras.models import Model
from keras.layers import concatenate, Dense, Input, GlobalAveragePooling2D
from keras.optimizers import Adam
from keras.optimizers import SGD
from keras.callbacks import EarlyStopping,ModelCheckpoint
from keras.initializers import glorot_uniform

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from keras import backend as K

model1 = load_model('/content/drive/MyDrive/TeaLeafNet26Mar-8_update_resnet50login.h5')

class EnsembleGenerator(Sequence):
    def __init__(self, gen1, gen2, gen3):
        self.gen1 = gen1
        self.gen2 = gen2
        self.gen3 = gen3

    def __len__(self):
        return len(self.gen1)

    def __getitem__(self, idx):
        X1, y1 = self.gen1[idx]
        X2, y2 = self.gen2[idx]
        X3, y3 = self.gen3[idx]
        return [X1, X2, X3], y1  # or y2 or y3, since they should all have the same labels

true_labels=train_batches_vgg16.classes
print("true label shape : ",true_labels.shape)
test_generator=EnsembleGenerator(train_batches_googlenet,train_batches_vgg16,train_batches_resnet)



input_layer = model1.input
output_layer = model1.layers[-2].output
new_model = Model(inputs=input_layer, outputs=output_layer)
new_model.summary()
features=new_model.predict(test_generator)

print("features shape : ", features.shape)

#apply pca
n_components = 65
pca = PCA(n_components=n_components)
pca.fit(features)
transformed_data = pca.transform(features)
print("transformed_data.shape : ",transformed_data.shape)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score

def evaluate_classification(true_labels, pred_labels, class_labels_integer, class_labels_actual):
    # Generate confusion matrix
    cm = confusion_matrix(true_labels, pred_labels, labels=class_labels_integer)

    # Generate classification report
    cr = classification_report(true_labels, pred_labels, labels=class_labels_integer, target_names=class_labels_actual, output_dict=True)
    print(classification_report(true_labels, pred_labels, labels=class_labels_integer, target_names=class_labels_actual))
    # Extract precision, recall, accuracy, and f1-score from classification report
    precision = [cr[class_labels_actual[label]]['precision'] for label in range(len(class_labels_actual))]
    recall = [cr[class_labels_actual[label]]['recall'] for label in range(len(class_labels_actual))]
    f1 = [cr[class_labels_actual[label]]['f1-score'] for label in range(len(class_labels_actual))]
    accuracy = accuracy_score(true_labels, pred_labels)

    # Plot bar graph of precision, recall, accuracy, and f1-score
    x = np.arange(len(class_labels_actual))
    width = 0.2
    fig, ax = plt.subplots(figsize=(12, 12))
    rects1 = ax.bar(x - width, precision, width, label='Precision')
    rects2 = ax.bar(x, recall, width, label='Recall')
    rects3 = ax.bar(x + width, f1, width, label='F1-score')
    ax.set_xticks(x)
    ax.set_xticklabels(class_labels_actual)
    ax.set_xlabel('Class Labels')
    ax.set_ylabel('Score')
    ax.set_title('Classification Metrics')
    ax.legend()

    # Plot bar graph of accuracy, precision, recall, and f1-score
    fig2, ax2 = plt.subplots(figsize=(8, 6))
    metrics = [accuracy, precision_score(true_labels, pred_labels, average='macro'), recall_score(true_labels, pred_labels, average='macro'), f1_score(true_labels, pred_labels, average='macro')]
    labels2 = ['Accuracy', 'Precision', 'Recall', 'F1-score']
    rects4 = ax2.bar(labels2, metrics)
    ax2.set_ylabel('Score')
    ax2.set_title('Aggregate Metrics')

    print('Accuracy: ',accuracy)
    print('Precision',precision_score(true_labels, pred_labels, average='macro'))
    print('Recall : ',recall_score(true_labels, pred_labels, average='macro'))
    print('F1 score : ',f1_score(true_labels, pred_labels, average='macro'))

    # Plot confusion matrix as a heatmap
    fig3, ax3 = plt.subplots(figsize=(8, 8))
    im = ax3.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    ax3.set(xticks=np.arange(cm.shape[1]),
            yticks=np.arange(cm.shape[0]),
            xticklabels=class_labels_actual, yticklabels=class_labels_actual,
            title='Confusion matrix',
            ylabel='True label',
            xlabel='Predicted label')
    plt.setp(ax3.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax3.text(j, i, format(cm[i, j], 'd'),
                     ha="center", va="center",
                     color="white" if cm[i, j] > cm.max() / 2. else "black")
    fig3.tight_layout()
     # Calculate and plot ROC curve
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(len(class_labels_actual)):
        fpr[i], tpr[i], _ = roc_curve((true_labels == i), (pred_labels == i))
        roc_auc[i] = auc(fpr[i], tpr[i])
    fig2, ax2 = plt.subplots(figsize=(8, 6))
    for i in range(len(class_labels_actual)):
        ax2.plot(fpr[i], tpr[i], label='{} (AUC = {:.2f})'.format(class_labels_actual[i], roc_auc[i]))
    ax2.plot([0, 1], [0, 1], 'k--')
    ax2.set_xlabel('False Positive Rate')
    ax2.set_ylabel('True Positive Rate')
    ax2.set_title('Receiver Operating Characteristic (ROC) Curve')
    ax2.legend()


    return fig, (precision, recall, f1, accuracy), fig2

#RF

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import pandas as pd

def apply_random_forest(data, labels):
    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)

    # Create a random forest classifier with 100 trees
    clf = RandomForestClassifier(n_estimators=100, random_state=42)

    # Fit the algorithm to the training data
    clf.fit(X_train, y_train)

    # Predict the labels for the test data
    y_pred = clf.predict(X_test)

    # Get training accuracy
    train_accuracy = clf.score(X_train, y_train)
    print("Training accuracy:", train_accuracy)

    # Print the accuracy score of the algorithm
    #print("Accuracy:", clf.score(X_test, y_test))
    class_labels = clf.classes_
    #print("class_labels : ",class_labels)
    #print("y_pred :",y_pred)
    #print("y_test : ",y_test)
    evaluate_classification(y_test.astype(int),y_pred.astype(int),class_labels,list(train_batches_vgg16.class_indices.keys()))
    return y_pred

apply_random_forest(transformed_data,true_labels)

#SVM

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

def apply_svm(data, labels):
    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)

    # Create SVM model and fit to training data
    model = SVC(kernel='linear')
    model.fit(X_train, y_train)

    # Evaluate model on test data
    accuracy = model.score(X_test, y_test)
    print("accuracy : ",accuracy)
    # Get predictions for test data
    train_accuracy = model.score(X_train, y_train)
    print("Training accuracy:", train_accuracy)

    predictions = model.predict(X_test)
    class_labels = model.classes_
    evaluate_classification(y_test.astype(int),predictions.astype(int),class_labels,list(train_batches_vgg16.class_indices.keys()))
    return model, accuracy, predictions


apply_svm(transformed_data,true_labels)

#decision tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

def apply_decision_tree(data, labels):
    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)

    # Create decision tree model and fit to training data
    model = DecisionTreeClassifier()
    model.fit(X_train, y_train)

    # Evaluate model on test data
    accuracy = model.score(X_test, y_test)
    print("Accuracy:", accuracy)

    train_accuracy = model.score(X_train, y_train)
    print("Training accuracy:", train_accuracy)


    # Get predictions for test data
    predictions = model.predict(X_test)
    class_labels = model.classes_
    evaluate_classification(y_test.astype(int), predictions.astype(int), class_labels, list(train_batches_vgg16.class_indices.keys()))

    return model, accuracy, predictions

apply_decision_tree(transformed_data, true_labels)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

def apply_knn(data, labels):
    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)

    # Create KNN model and fit to training data
    model = KNeighborsClassifier(n_neighbors=5)
    model.fit(X_train, y_train)

    # Evaluate model on test data
    accuracy = model.score(X_test, y_test)
    print("accuracy : ",accuracy)

    train_accuracy = model.score(X_train, y_train)
    print("Training accuracy:", train_accuracy)


    # Get predictions for test data
    predictions = model.predict(X_test)
    class_labels = model.classes_
    evaluate_classification(y_test.astype(int),predictions.astype(int),class_labels,list(train_batches_vgg16.class_indices.keys()))
    return model, accuracy, predictions


apply_knn(transformed_data,true_labels)

#logisticregression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

def apply_logistic_regression(data, labels):
    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)

    # Create logistic regression model and fit to training data
    model = LogisticRegression(random_state=42)
    model.fit(X_train, y_train)

    # Evaluate model on test data
    accuracy = model.score(X_test, y_test)
    print("Accuracy:", accuracy)

    # Get predictions for test data
    predictions = model.predict(X_test)

    train_accuracy = model.score(X_train, y_train)
    print("Training accuracy:", train_accuracy)


    # Define class labels and integer labels
    class_labels = [str(i) for i in model.classes_]
    class_labels_int = [i for i in range(len(class_labels))]

    # Evaluate model using metrics
    evaluate_classification(y_test, predictions, class_labels_int, list(train_batches_vgg16.class_indices.keys()))

    return model, accuracy, predictions


apply_logistic_regression(transformed_data, true_labels)

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split

def apply_naive_bayes(data, labels):
    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)

    # Create Naive Bayes model and fit to training data
    model = GaussianNB()
    model.fit(X_train, y_train)

    # Evaluate model on test data
    accuracy = model.score(X_test, y_test)
    print("Accuracy:", accuracy)

    # Get predictions for test data
    predictions = model.predict(X_test)

    train_accuracy = model.score(X_train, y_train)
    print("Training accuracy:", train_accuracy)


    # Define class labels and integer labels
    class_labels = model.classes_
    class_labels_int = [i for i in range(len(class_labels))]

    # Evaluate model using metrics
    evaluate_classification(y_test, predictions, class_labels_int, list(train_batches_vgg16.class_indices.keys()))

    return model, accuracy, predictions

apply_naive_bayes(transformed_data, true_labels)

#Linear Discriminant Analysis (LDA)



from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import train_test_split

def apply_lda(data, labels):
    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)

    # Create LDA model and fit to training data
    model = LinearDiscriminantAnalysis()
    model.fit(X_train, y_train)

    # Evaluate model on test data
    accuracy = model.score(X_test, y_test)
    print("Accuracy:", accuracy)

    # Get predictions for test data
    predictions = model.predict(X_test)

    train_accuracy = model.score(X_train, y_train)
    print("Training accuracy:", train_accuracy)


    # Define class labels and integer labels
    class_labels = model.classes_
    print("class_labels : ",class_labels)

    class_labels_int = [i for i in range(len(class_labels))]
    print("class_labels_int : ",class_labels_int)

    # Evaluate model using metrics
    evaluate_classification(y_test, predictions, class_labels_int, list(train_batches_vgg16.class_indices.keys()))

    return model, accuracy, predictions

apply_lda(transformed_data, true_labels)